{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPcq8K8j05kJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "\n",
        "def load_mega_metrics(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"Normalised Weighted Spectral Distribution (Normalized Laplacian, K=40, N=4)\"] = np.real([complex(item) for item in df[combined_metrics][\"Normalised Weighted Spectral Distribution (Normalized Laplacian, K=40, N=4)\"].tolist()])\n",
        "    return df\n",
        "\n",
        "def get_columns(df):\n",
        "    return df.columns.tolist()\n",
        "\n",
        "\n",
        "def get_physical_selected_metrics():\n",
        "    selected_metrics = [\n",
        "                      'Diameter (hops)',\n",
        "                      'Average Shortest Path Length (Hops)',\n",
        "                      'Edge Density (Physical Connectivity)',\n",
        "                      ]\n",
        "    return selected_metrics\n",
        "\n",
        "def get_spectral_selected_metrics():\n",
        "    selected_metrics = [\n",
        "                      'Normalized Spectral Radius (Un-weighted Adjacency Matrix)',\n",
        "                      'Normalised Algebraic Connectivity (Normalized Laplacian)',\n",
        "                      'Normalised Weighted Spectral Distribution (Normalized Laplacian, K=40, N=4)'\n",
        "                      ]\n",
        "    return selected_metrics\n",
        "\n",
        "def get_spatial_selected_metrics():\n",
        "    selected_metrics = [\n",
        "                      'Normalized Average Link Length ',\n",
        "                      'Normalized Diameter (Link Lengths)',\n",
        "                      'Normalized Average Shortest Path Length (Link Lengths)'\n",
        "                      ]\n",
        "    return selected_metrics\n",
        "\n",
        "def get_combined_selected_metrics():\n",
        "    selected_metrics = [\n",
        "                      'Diameter (hops)',\n",
        "                      'Average Shortest Path Length (Hops)',\n",
        "                      'Edge Density (Physical Connectivity)',\n",
        "                      'Normalized Spectral Radius (Un-weighted Adjacency Matrix)',\n",
        "                      'Normalised Algebraic Connectivity (Normalized Laplacian)',\n",
        "                      'Normalised Weighted Spectral Distribution (Normalized Laplacian, K=40, N=4)',\n",
        "                      'Normalized Average Link Length ',\n",
        "                      'Normalized Diameter (Link Lengths)',\n",
        "                      'Normalized Average Shortest Path Length (Link Lengths)',\n",
        "                      ]\n",
        "    return selected_metrics\n",
        "\n",
        "def get_numeric_features(data, selected_metrics):\n",
        "    # Select relevant features for clustering\n",
        "    # data is dataframe and metrics is the set of selected metrics\n",
        "    # Select metrics from df\n",
        "    features = data[selected_metrics].select_dtypes(include=[np.number])\n",
        "    print(features)\n",
        "    return features\n",
        "\n",
        "def apply_k_means_clustering(data, features, n_clusters = 3, n_init = 10, random_state = 42, max_iter = 300):\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters = n_clusters, n_init = n_init, random_state = random_state, max_iter = max_iter)\n",
        "    clusters = kmeans.fit_predict(features)\n",
        "    data['Cluster'] = clusters\n",
        "\n",
        "    # Create a table of category names and assigned topologies\n",
        "    clustered_topologies = data.groupby('Cluster')['Topology Name'].apply(list).reset_index()\n",
        "    clustered_topologies['Cluster'] = clustered_topologies['Cluster'].apply(lambda x: f'Class {x+1}')\n",
        "    clustered_topologies.columns = ['Category Name', 'Topologies Assigned']\n",
        "    clustered_topologies.to_csv('clustered_topologies.csv', index=False)\n",
        "\n",
        "    return clusters, clustered_topologies\n",
        "\n",
        "def compute_silhouette_avg(features, clusters):\n",
        "    # Compute silhouette score\n",
        "    silhouette_avg = silhouette_score(features, clusters)\n",
        "    print(silhouette_avg)\n",
        "    return silhouette_avg\n",
        "\n",
        "def standardise_features(features, scaler_type = 'standard'):\n",
        "    if scaler_type == 'standard':\n",
        "      scaler = StandardScaler()\n",
        "    elif scaler_type == 'minmax':\n",
        "      scaler = MinMaxScaler()\n",
        "    elif scaler == 'robust':\n",
        "      scaler = RobustScaler()\n",
        "    # Scale Features according to chosen type\n",
        "    scaled_features = scaler.fit_transform(features)\n",
        "    return scaled_features\n",
        "\n",
        "def apply_pca(features, n_components = 2):\n",
        "    # Apply PCA for 2D visualisation\n",
        "    # Apply PCA to reduce dimensions to 2 for visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_features = pca.fit_transform(features)\n",
        "    # Get the explained variance ratio\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "    # Calculate cumulative explained variance\n",
        "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "    # Print the variance explained by the first two components\n",
        "    print(\"Variance explained by first two components:\")\n",
        "    print(f\"PC1: {explained_variance_ratio[0]:.2f}\")\n",
        "    print(f\"PC2: {explained_variance_ratio[1]:.2f}\")\n",
        "    print(f\"Total: {explained_variance_ratio[0] + explained_variance_ratio[1]:.2f}\")\n",
        "\n",
        "    # Print cumulative variance explained\n",
        "    print(\"\\nCumulative variance explained:\")\n",
        "    for i, var in enumerate(cumulative_variance_ratio[:5], 1):\n",
        "        print(f\"First {i} PCs: {var:.2f}\")\n",
        "    return pca_features\n",
        "\n",
        "\n",
        "\n",
        "# Function to compute margin for linear SVM\n",
        "def compute_margin(svc):\n",
        "    if svc.kernel == 'linear':\n",
        "        return 1 / np.sqrt(np.sum(svc.coef_ ** 2))\n",
        "    return None\n",
        "\n",
        "# Function to perform cross-validation and compute mean accuracy and mean margin\n",
        "def cross_val_svm_with_mean_margin(pca_features, clusters, cv=5, kernel='linear'):\n",
        "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    accuracies = []\n",
        "    margins = []\n",
        "\n",
        "    for train_index, test_index in kf.split(pca_features):\n",
        "        X_train, X_test = pca_features[train_index], pca_features[test_index]\n",
        "        y_train, y_test = clusters[train_index], clusters[test_index]\n",
        "\n",
        "        svc = SVC(kernel=kernel, probability=True)\n",
        "        svc.fit(X_train, y_train)\n",
        "\n",
        "        # Compute accuracy for this fold\n",
        "        accuracy = svc.score(X_test, y_test)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        # Compute margin for this fold\n",
        "        margin = compute_margin(svc)\n",
        "        if margin is not None:\n",
        "            margins.append(margin)\n",
        "\n",
        "    # Calculate mean accuracy and mean margin\n",
        "    cv_mean_accuracy = np.mean(accuracies)\n",
        "    mean_margin = np.mean(margins) if margins else None\n",
        "\n",
        "    return cv_mean_accuracy, mean_margin\n",
        "\n",
        "def get_decision_boundaries(pca_features, clusters, kernel='linear', degree=3):\n",
        "    # Train the SVM classifier\n",
        "    svc = SVC(kernel=kernel, degree=degree, probability=True)\n",
        "    svc.fit(pca_features, clusters)\n",
        "\n",
        "    # Predict clusters for the PCA features to calculate metrics\n",
        "    predicted_clusters = svc.predict(pca_features)\n",
        "    accuracy = accuracy_score(clusters, predicted_clusters)\n",
        "    f1 = f1_score(clusters, predicted_clusters, average='weighted')\n",
        "    precision = precision_score(clusters, predicted_clusters, average='weighted')\n",
        "    recall = recall_score(clusters, predicted_clusters, average='weighted')\n",
        "    auc_roc = roc_auc_score(clusters, svc.predict_proba(pca_features), multi_class='ovr')\n",
        "    conf_matrix = confusion_matrix(clusters, predicted_clusters)\n",
        "\n",
        "    # Calculate the decision function margins\n",
        "    if kernel == 'linear':\n",
        "        margin = 1 / np.sqrt(np.sum(svc.coef_ ** 2))\n",
        "    else:\n",
        "        margin = None  # Margins are typically calculated for linear kernels\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"AUC-ROC:\", auc_roc)\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "    print(\"Margin:\", margin)\n",
        "\n",
        "    # Calculate cross-validation score - computes both mean acc and mean margin\n",
        "    cv_mean_accuracy, cv_mean_margin = cross_val_svm_with_mean_margin(pca_features, clusters, cv=5, kernel='linear')\n",
        "    print(\"Cross-Validation Mean Accuracy:\", cv_mean_accuracy)\n",
        "    print(\"Cross-Validation Mean Margin:\", cv_mean_margin)\n",
        "\n",
        "    # Create a mesh grid for plotting decision boundaries\n",
        "    x_min, x_max = pca_features[:, 0].min() - 1, pca_features[:, 0].max() + 1\n",
        "    y_min, y_max = pca_features[:, 1].min() - 1, pca_features[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    # Plot decision boundaries\n",
        "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    return xx, yy, Z, svc, accuracy, f1, precision, recall, auc_roc, conf_matrix, margin, cv_mean_accuracy, cv_mean_margin\n",
        "\n",
        "\n",
        "\n",
        "# Increased Quality for Journal\n",
        "def plot_the_clusters_with_decision_boundaries(xx, yy, Z, pca_features, clusters, filename='clusters'):\n",
        "    plt.figure(figsize=(10, 8), dpi=300)  # Set figure size and DPI for higher quality\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "\n",
        "    scatter = plt.scatter(pca_features[:, 0], pca_features[:, 1], c=clusters, edgecolor='k', s=100)\n",
        "    # Extract the colors used by the scatter plot\n",
        "    handles, labels = scatter.legend_elements()\n",
        "    labels = [f'Cluster {int(label.split(\"{\")[1].split(\"}\")[0])}' for label in labels]\n",
        "\n",
        "    # plt.title('KMeans Clustering with SVC Decision Boundaries')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.legend(handles, labels)  # Add legend with extracted colors and labels\n",
        "    plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')  # Save the figure with high DPI\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def multiple_k_mean_runs(data, features, n_clusters=3, random_state_list = [42, 7, 108], n_init = 10, max_iter = 300, filename = 'experiement_'):\n",
        "    clusters_list = []\n",
        "    clustered_topologies_list = []\n",
        "    # Apply K-Means clustering\n",
        "    for random_state in random_state_list:\n",
        "        kmeans = KMeans(n_clusters = n_clusters, n_init = n_init, random_state = random_state, max_iter = max_iter)\n",
        "        clusters = kmeans.fit_predict(features)\n",
        "        clusters_list.append(clusters)\n",
        "        data['Cluster'] = clusters\n",
        "\n",
        "        # Create a table of category names and assigned topologies\n",
        "        clustered_topologies = data.groupby('Cluster')['Topology Name'].apply(list).reset_index()\n",
        "        clustered_topologies['Cluster'] = clustered_topologies['Cluster'].apply(lambda x: f'Class {x+1}')\n",
        "        clustered_topologies.columns = ['Category Name', 'Topologies Assigned']\n",
        "        clustered_topologies_list.append(clustered_topologies)\n",
        "        clustered_topologies.to_csv(filename + 'clustered_topologies_random_state_' + str(random_state) + '.csv', index=False)\n",
        "\n",
        "    return clusters_list, clustered_topologies_list\n",
        "\n",
        "\n",
        "def get_score_and_plots(selected_metrics, file_path = '../../data/real/mega_graph_metrics.csv',  n_clusters=3,\n",
        "                        random_state_list = [42, 7, 108], n_init = 50, max_iter = 500, filename = 'experiment_metrics_',  kernel = 'linear', image_name = 'clusters_for_metrics'):\n",
        "    # Load the dataset\n",
        "    df = load_mega_metrics(file_path)\n",
        "\n",
        "    # Get numeric features\n",
        "    numeric_features = get_numeric_features(df, selected_metrics)\n",
        "\n",
        "    # # Standardize features\n",
        "    scaled_features = standardise_features(numeric_features, scaler_type='standard')\n",
        "\n",
        "    # Apply PCA on standardised features\n",
        "    pca_features = apply_pca(scaled_features, n_components = 2)\n",
        "\n",
        "    # Multiple k means on scaled features\n",
        "    clusters_list, clustered_topologies_list = multiple_k_mean_runs(df,\n",
        "                                                                    pca_features,\n",
        "                                                                    n_clusters=n_clusters,\n",
        "                                                                    random_state_list = random_state_list,\n",
        "                                                                    n_init = n_init,\n",
        "                                                                    max_iter = max_iter,\n",
        "                                                                    filename = filename)\n",
        "\n",
        "\n",
        "    # Compute Silhoutte scores\n",
        "    silhouette_avg_list = [compute_silhouette_avg(pca_features, clusters_list[0]),\n",
        "                           compute_silhouette_avg(pca_features, clusters_list[1]),\n",
        "                           compute_silhouette_avg(pca_features, clusters_list[2])]\n",
        "\n",
        "\n",
        "    # Get decision boundaries on pca features\n",
        "    xx, yy, Z, svc, accuracy, f1, precision, recall, auc_roc, conf_matrix, margin, cv_mean_accuracy, cv_mean_margin = get_decision_boundaries(pca_features,\n",
        "                                            clusters_list[0],\n",
        "                                            kernel = kernel)\n",
        "\n",
        "    # Plot the clusters computed with scaled features\n",
        "    plot_the_clusters_with_decision_boundaries(xx, yy, Z,\n",
        "                                              pca_features,\n",
        "                                              clusters_list[0],\n",
        "                                              filename=image_name)\n",
        "\n",
        "\n",
        "    return clusters_list, clustered_topologies_list, silhouette_avg_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRJBeH5p4Mao"
      },
      "outputs": [],
      "source": [
        "spectral_metrics = get_spectral_selected_metrics()\n",
        "spatial_metrics = get_spatial_selected_metrics()\n",
        "combined_metrics = get_combined_selected_metrics()\n",
        "physical_metrics = get_physical_selected_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH-ne7OtOVtO",
        "outputId": "1fa71b35-9734-49d1-a43b-d0d09c6dc888"
      },
      "outputs": [],
      "source": [
        "print(\"------------Spectral Metrics-------------\")\n",
        "print(spectral_metrics)\n",
        "print(\"------------Spatial Metrics-------------\")\n",
        "print(spatial_metrics)\n",
        "print(\"------------Physical Metrics-------------\")\n",
        "print(physical_metrics)\n",
        "print(\"------------Combined Metrics-------------\")\n",
        "print(combined_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bAJo5YBE4lWT",
        "outputId": "bd18dcb7-93e5-43bd-e3f2-738a6c02d387"
      },
      "outputs": [],
      "source": [
        "spectral_clusters_list, spectral_clustered_topologies_list, spectral_silhouette_avg_list = get_score_and_plots(spectral_metrics,\n",
        "                                                                                                               n_clusters=3,\n",
        "                                                                                                               n_init = 150,\n",
        "                                                                                                               max_iter = 1500 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K9prEXbN49tq",
        "outputId": "78302117-52bd-4ef3-fe60-08fed28bbbf9"
      },
      "outputs": [],
      "source": [
        "spatial_clusters_list, spatial_clustered_topologies_list, spatial_silhouette_avg_list = get_score_and_plots(spatial_metrics,\n",
        "                                                                                                               n_clusters=3,\n",
        "                                                                                                               n_init = 150,\n",
        "                                                                                                               max_iter = 1500 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UGR-LNFd5qM4",
        "outputId": "2964e102-a4b3-4046-aac4-b76ce529336d"
      },
      "outputs": [],
      "source": [
        "physical_clusters_list, physical_clustered_topologies_list, physical_silhouette_avg_list = get_score_and_plots(physical_metrics,\n",
        "                                                                                                               n_clusters=3,\n",
        "                                                                                                               n_init = 50,\n",
        "                                                                                                               max_iter = 1500 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M7Su0eH-52MZ",
        "outputId": "50503a77-3ebe-44e6-b526-0619c97a0d73"
      },
      "outputs": [],
      "source": [
        "combined_clusters_list, combined_clustered_topologies_list,  combined_silhouette_avg_list = get_score_and_plots( combined_metrics,\n",
        "                                                                                                               n_clusters=3,\n",
        "                                                                                                               n_init = 150,\n",
        "                                                                                                               max_iter = 1500 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl1gerI0kL31",
        "outputId": "5f6654a4-06fe-4f66-d1df-cb1ff311ead7"
      },
      "outputs": [],
      "source": [
        "spectral_clustered_topologies_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bscPUDj7keJ-",
        "outputId": "95a73f87-7681-4598-eede-3e3ad026d4e1"
      },
      "outputs": [],
      "source": [
        "spatial_clustered_topologies_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJJhiZw4ko7a",
        "outputId": "cb277c28-2102-4794-cf52-5766be3a2b41"
      },
      "outputs": [],
      "source": [
        "physical_clustered_topologies_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE-Wv4Hqk2Yj",
        "outputId": "9757038c-da19-4712-b501-7925920305ce"
      },
      "outputs": [],
      "source": [
        "combined_clustered_topologies_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKbj9H791FfJ"
      },
      "outputs": [],
      "source": [
        "spectral_clustered_topologies_list[0].to_csv('spectral_clustered_topologies_random_seed_42.csv', index=False)\n",
        "spectral_clustered_topologies_list[1].to_csv('spectral_clustered_topologies_random_seed_7.csv', index=False)\n",
        "spectral_clustered_topologies_list[2].to_csv('spectral_clustered_topologies_random_seed_108.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMF3U6GBj_q6"
      },
      "outputs": [],
      "source": [
        "spatial_clustered_topologies_list[0].to_csv('spatial_clustered_topologies_random_seed_42.csv', index=False)\n",
        "spatial_clustered_topologies_list[1].to_csv('spatial_clustered_topologies_random_seed_7.csv', index=False)\n",
        "spatial_clustered_topologies_list[2].to_csv('spatial_clustered_topologies_random_seed_108.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8hSmwEgkF8p"
      },
      "outputs": [],
      "source": [
        "physical_clustered_topologies_list[0].to_csv('physical_clustered_topologies_random_seed_42.csv', index=False)\n",
        "physical_clustered_topologies_list[1].to_csv('physical_clustered_topologies_random_seed_7.csv', index=False)\n",
        "physical_clustered_topologies_list[2].to_csv('physical_clustered_topologies_random_seed_108.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is8kMJpVkT_D"
      },
      "outputs": [],
      "source": [
        "combined_clustered_topologies_list[0].to_csv('combined_clustered_topologies_random_seed_42.csv', index=False)\n",
        "combined_clustered_topologies_list[1].to_csv('combined_clustered_topologies_random_seed_7.csv', index=False)\n",
        "combined_clustered_topologies_list[2].to_csv('combined_clustered_topologies_random_seed_108.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z55M58hlFWO"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary for each set of metrics\n",
        "metrics = {\n",
        "    'Metrics': ['Spectral', 'Spatial', 'Physical', 'Combined'],\n",
        "    'Variance PC1': [0.47, 0.83, 0.87, 0.49],\n",
        "    'Variance PC2': [0.36, 0.15, 0.12, 0.26],\n",
        "    'Total Variance': [0.83, 0.98, 0.99, 0.76],\n",
        "    'Accuracy': [0.9809523809523809, 1.0, 1.0, 1.0],\n",
        "    'F1 Score': [0.9809567528237605, 1.0, 1.0, 1.0],\n",
        "    'Precision': [0.9817006802721088, 1.0, 1.0, 1.0],\n",
        "    'Recall': [0.9809523809523809, 1.0, 1.0, 1.0],\n",
        "    'AUC-ROC': [1.0, 1.0, 1.0, 1.0],\n",
        "    'Margin': [0.22107128158757422, 0.23189128709663, 0.23769533691391528, 0.2765939198212106]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "df_metrics.to_csv('clustering_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj8JjX4VO16k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
